{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image Processing: Principal Component Analysis (PCA)**\n",
    "\n",
    "In this Jupyter Notebook, we will implement Principal Compomnent Analysis (PCA) using NumPy, and conduct some experiments that are similar to the \"EigenFace\" study.\n",
    "\n",
    "We use the sign language image data. What we are basically doing is to select the image data of one class (e.g., the sign for \"zero\"), and extract its principal components (PCs). Then we can visualize the data by reducing the dimension to the first 2 most dominant PCs. Also, given a new image, we can use its projections onto the PCs to determine whether it is a sign for \"zero\" or not.\n",
    "\n",
    "***\n",
    "\n",
    "## **1. Load Data**\n",
    "\n",
    "Load data and preprocess.\n",
    "\n",
    "The dimension of data $X$ is $n\\times m$, in which $n=4096=64\\times64$ is the total number of pixels in an image, and $m=163$ is the number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.load(open('../data/X_train.npy', 'rb'))\n",
    "Y_all = np.load(open('../data/Y_train.npy', 'rb'))\n",
    "# print(X_all.shape)\n",
    "# print(Y_all.shape)\n",
    "# print(Y_all[162,]) # y=0\n",
    "# print(Y_all[163,]) # y=1\n",
    "# print(Y_all[327,]) # y=2\n",
    "\n",
    "X_reshape = np.transpose(X_all, (1,2,3,0)).reshape(-1, X_all.shape[0])\n",
    "X = X_reshape[:, :163]\n",
    "print('X shape:', X.shape)\n",
    "\n",
    "# Print some sample images\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "for i, idx in enumerate([0, 50, 100]):\n",
    "    fig.add_subplot(1, 3, i+1)\n",
    "    img = X_all[idx,:,:,0]\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Sample image {}'.format(i+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Centerize Data**\n",
    "\n",
    "We need to centerize the data by subtracting $X$ from the mean values of each row. I.e., for each $i,j$, $X[i,j]=mean(X[i,:])$.\n",
    "\n",
    "This step is necessary before carrying out PCA. Notice that in the slides we do a further normalization by dividing by the standard deviation after subtracting the means. In this notebook, we do not use the dividing step for the convenience of later reconstruction steps. Fortunately, not fully normalizing the data will not harm PCA of image data.\n",
    "\n",
    "- We can use `np.mean()` to compute the mean of each row, with argument `axis=1`\n",
    "- The result of computing `np.mean` on $X$ (of shape (n, m)) could be (n,), and that means we may need to reshape it to (n,1) before subtracting it from $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tilde = X - np.mean(X, axis=1).reshape(-1, 1)\n",
    "\n",
    "assert X.shape == X_tilde.shape\n",
    "print('Shape of the centerized data:', X_tilde.shape)\n",
    "print('First 5 elements of first row in X_tilde:', X_tilde[0,:5])\n",
    "print('First 5 elements of last row in X_tilde:', X_tilde[-1,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## **3. Construct Covariance Matrix and Compute Eigenvectors**\n",
    "\n",
    "The principal components of data can be obtained by computing the eigenvectors $u_i$ of the covariance matrix $\\frac{1}{m}\\tilde{X}\\tilde{X}^{T}$.\n",
    "\n",
    "However, the shape of $\\frac{1}{m}\\tilde{X}\\tilde{X}^{T}$ is $n\\times n$, and it will be slow to carry out eigen decomposition when $n$ is big (In our case, $n=64\\times 64=4096$). Therefore, we need to use an alternative way.\n",
    "\n",
    "The **solution** is that we first compute the eigenvectors $u'_i$ of the matrix $\\frac{1}{m}\\tilde{X}^{T}\\tilde{X}$ (of shape $m\\times m$), which satisfy the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\big(\\frac{1}{m}\\tilde{X}^{T}\\tilde{X}\\big)u'_i = \\lambda_i u'_i\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda_i$ is the correponding non-zero eigenvalue. Note that $\\lambda_i/\\sum{\\lambda_i}$ measures the proportion of variance that the $i$th principal component explains.\n",
    "Because in our case $m=163$ is much smaller than $n=4096$, the computation for $u'_i$ is much faster. Then we can compute the eigenvectors of $\\frac{1}{m}\\tilde{X}\\tilde{X}^{T}$ by: $u_i = \\tilde{X}u'_i$. This is valid because of the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\big(\\frac{1}{m}\\tilde{X}^{T}\\tilde{X}\\big)u'_i = \\lambda_i u'_i \\Longrightarrow \\tilde{X}\\big(\\frac{1}{m}\\tilde{X}^{T}\\tilde{X}\\big)u'_i = \\tilde{X}\\lambda_i u'_i \\Longrightarrow \\big(\\frac{1}{m}\\tilde{X}\\tilde{X}^{T}\\big)(\\tilde{X}u'_i) = \\lambda_i (\\tilde{X}u'_i)\n",
    "\\end{equation}\n",
    "\n",
    "Of course we need to normalize $u_i$ by $u_i = \\frac{u_i}{||u_i||}$, so that it satisfy the $||u||=1$ constraint of principal components.\n",
    "\n",
    "- We can compute the eigenvectors of a $m\\times m$ matrix by calling `numpy.linalg.eig()`. It returns two numpy arrays `w` and `v`, among which `w` contains $m$ eigenvalues, and `v` is a $m\\times m$ array whose ith column `v[:,i]` is the corresponding eigenvector of `w[i]`  \n",
    "- We can normalize a vector by calling `numpy.linalg.norm()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the covariance matrix for computing u'_i\n",
    "covmat = (np.dot(X_tilde.T, X_tilde)) / X_tilde.shape[1]\n",
    "\n",
    "# Compute u'_i, which is stored in the variable v\n",
    "w, v = np.linalg.eig(covmat)\n",
    "\n",
    "# Compute u_i from u'_i, and store it in the variable U\n",
    "U = np.dot(X_tilde, v)\n",
    "\n",
    "# Normalize u_i, i.e., each column of U\n",
    "U = U / np.linalg.norm(U, axis=0)\n",
    "\n",
    "# Evaluate eigenvalues\n",
    "ratios = w / np.sum(w)\n",
    "print('PC1 explains {}% of the total variance'.format(ratios[0]))\n",
    "print('PC2 explains {}% of the total variance'.format(ratios[1]))\n",
    "print('First 100 PCs explains {}% of the total variance'.format(sum(ratios[:100])))\n",
    "print()\n",
    "\n",
    "# Evaluate U\n",
    "print('Shape of U:', U.shape)\n",
    "print('First 5 elements of first column of U:', U[:5,0])\n",
    "print('First 5 elements of last column of U:', U[:5,-1])\n",
    "\n",
    "# Plot eigenvectors as if they are image data, i.e., eigenhands\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "for i in range(3):\n",
    "    img = U[:, i].reshape((64, 64))\n",
    "    fig.add_subplot(1, 3, i+1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('PC' + str(i+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## **4. Reduce Dimension and Visualize**\n",
    "\n",
    "After we have obtained all the principal components, we can conduct dimensionality reduction by project the original data to a selected set of PCs. For visualization purpose, we select the top two PCs, and reduce the data to a 2-D space.\n",
    "\n",
    "Let $u_1$ and $u_2$ the 1st and 2nd columns of matrix `U`, the projection of an image data $\\tilde{x}$ (already centerized) can be done by computing the inner products $y_1 = u_1^T\\tilde{x}$ and $y_2 = u_2^T\\tilde{x}$. Then we can visualize the data using the scatter plot of $y_1$ and $y_2$.\n",
    "\n",
    "- We select the first two PCs by `U[:,:2]`, and then use `numpy.dot()` to compute the inner product with $\\tilde{X}$.\n",
    "- `Y` is of shape $2\\times m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.dot(U[:, :2].T, X_tilde)\n",
    "\n",
    "print('Shape of Y:', Y.shape)\n",
    "print('First 5 elements of first row of Y:', Y[0,:5])\n",
    "print('First 5 elements of second row of Y:', Y[1,:5])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(Y[0,:], Y[1,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## **5. Project and Reconstruct**\n",
    "\n",
    "We can not only project data onto principal components to reduce dimensions, but also use the projections to reconstruct data. \n",
    "\n",
    "Let $\\tilde{x}$ be the original data, and its projections on the first $k$ PCs are: $p_1 = u_1^T\\tilde{x},p_2 = u_2^T\\tilde{x},\\dots,p_k = u_k^T\\tilde{x}$. Then we can have the reconstruction of data $\\tilde{x}'$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{x}' = p_1\\cdot u_1 + p_2\\cdot u_2 + \\dots + p_k\\cdot u_k\n",
    "\\end{equation}\n",
    "\n",
    "If the full set of PCs are used, then the reconstruction will be perfect, i.e., exactly the same as the original image without losing any informaiton. If a subset (e.g., top $k$ PCs) is used, then the reconstruction will cause some information loss. \n",
    "\n",
    "This information loss can be measured by the Euclidean distance between the original data $\\tilde{x}$ and the reconstruted data $\\tilde{x}'$. Larger distance indicates higher information loss.\n",
    "\n",
    "- We will use `numpy.dot()` to compute projections and reconstructions.\n",
    "- We will use `numpy.linalg.norm()` to compute distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project and reconstruct\n",
    "def project_reconstruct(X, U, num_dims):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- centerized image data, of shape (n, m)\n",
    "    U -- numpy array containing principal components, of shape (n, m)\n",
    "    num_dims -- the number of PCs upon which to project X.\n",
    "    \n",
    "    Return:\n",
    "    X_proj -- projections on the top num_dims PCs, of shape (num_dims, m)\n",
    "    X_recon -- reconstructed data, of shape (n, m)\n",
    "    dist -- Euclidean distance between X and X_recon, of shape (m,)\n",
    "    \"\"\"\n",
    "    assert num_dims <= U.shape[1]\n",
    "    assert X.shape[0] == U.shape[0]\n",
    "    \n",
    "    X_proj = np.dot(U[:, :num_dims].T, X)\n",
    "    X_recon = np.dot(U[:, :num_dims], X_proj)\n",
    "    dist = np.linalg.norm(X - X_recon, axis=0)\n",
    "    \n",
    "    return X_proj, X_recon, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.add_subplot(1, 4, 1)\n",
    "img_orig = X_tilde[:,0].reshape((64, 64))\n",
    "plt.imshow(img_orig, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Original')\n",
    "\n",
    "for i, num_dims in enumerate([163, 100, 50]):\n",
    "    X_proj, X_recon, dist = project_reconstruct(X_tilde, U, num_dims)\n",
    "    print('Mean distance of reconstruction using {} PCs: {}'.format(num_dims, np.mean(dist)))\n",
    "    \n",
    "    fig.add_subplot(1, 4, i+2)\n",
    "    img_recon = X_recon[:,0].reshape((64, 64))\n",
    "    plt.imshow(img_recon, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Recon using {} PCs'.format(num_dims))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## **6. New Image Detection**\n",
    "\n",
    "Given a new image of the sign for a different number, if we project it onto the PCs that we extracted above, then the reconstruction distance will be very large, and the reconstructed image will not likely be recognizable. This is because the principal components summarize current image class well, but not apply to images of different classes. \n",
    "\n",
    "This is also how PCA can be used in applications such as face detection etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Project and reconstruct one instance of new data\n",
    "x_new = X_reshape[:, 163]\n",
    "x_new_tilde = x_new - np.mean(X, axis=1)\n",
    "\n",
    "_, x_new_recon, dist = project_reconstruct(x_new_tilde, U, 163)\n",
    "print('Reconstruction distance for out-of-class data:', dist)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "img_orig = x_new_tilde.reshape((64, 64))\n",
    "plt.imshow(img_orig, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Original')\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "img_recon = x_new_recon.reshape((64, 64))\n",
    "plt.imshow(img_recon, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Recon using 163 PCs')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
